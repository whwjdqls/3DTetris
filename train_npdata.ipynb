{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "55807c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from torch.utils.data import Dataset\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "7f2fb4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\"cho2\",\"jeon2\",\"kim2\"]\n",
    "# names = [\"cho2\"]\n",
    "file_names = [\"./class1.npy\",\"./class2.npy\",\"./class3.npy\",\"./class4.npy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "1e723a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "data_pth = \"./customdata\"\n",
    "for name in names:\n",
    "    name_pth = os.path.join(data_pth, name)\n",
    "    for file_name in file_names:\n",
    "        file_pth = os.path.join(name_pth,file_name)\n",
    "        data.append(np.load(file_pth))\n",
    "\n",
    "data = np.concatenate(data,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "33e8c4b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6000, 64)"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "ee11fc6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 3., 3., 3.])"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:,0]-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "ae8fa14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_onehot(labels,num_classes):\n",
    "    out = np.zeros((labels.shape[0],num_classes))\n",
    "    for i in range(labels.shape[0]):\n",
    "        out[i][int(labels[i])]=1\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "53fddb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset_np(Dataset):\n",
    "    def __init__(self, np_array ,num_class):\n",
    "        self.data = np_array\n",
    "        self.labels = torch.tensor(to_onehot(self.data[:,0]-1,num_class),dtype=torch.float32)\n",
    "        self.keypoints = torch.tensor(self.data[:,1:], dtype=torch.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        keypoint = self.keypoints[idx]\n",
    "        label = self.labels[idx]\n",
    "        return keypoint, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "c350c38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset_np(data, num_class = 4)\n",
    "train_set, test_set = torch.utils.data.random_split(dataset, [5500,500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "d0479a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size = 128, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size = 128, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "eef34061",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[-0.0000, -0.0000, -0.0000,  ..., -0.0133,  0.0243,  0.0083],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0495,  0.0160,  0.0025],\n",
       "         [-0.0000, -0.0000, -0.0000,  ..., -0.0172,  0.0172,  0.0382],\n",
       "         ...,\n",
       "         [-0.0000, -0.0000, -0.0000,  ...,  0.0179,  0.0527,  0.0294],\n",
       "         [-0.0000, -0.0000, -0.0000,  ..., -0.0116,  0.0266,  0.0080],\n",
       "         [-0.0000, -0.0000, -0.0000,  ..., -0.0163,  0.0171,  0.0182]]),\n",
       " tensor([[0., 0., 1., 0.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [0., 0., 0., 1.],\n",
       "         [0., 1., 0., 0.],\n",
       "         [0., 0., 1., 0.],\n",
       "         [0., 1., 0., 0.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [0., 0., 1., 0.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [0., 0., 1., 0.],\n",
       "         [0., 1., 0., 0.],\n",
       "         [0., 0., 1., 0.],\n",
       "         [0., 1., 0., 0.],\n",
       "         [0., 0., 1., 0.],\n",
       "         [0., 0., 1., 0.],\n",
       "         [0., 0., 1., 0.],\n",
       "         [0., 0., 1., 0.],\n",
       "         [0., 1., 0., 0.],\n",
       "         [0., 0., 1., 0.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [0., 1., 0., 0.],\n",
       "         [0., 0., 1., 0.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [0., 1., 0., 0.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [0., 1., 0., 0.],\n",
       "         [0., 0., 0., 1.],\n",
       "         [0., 0., 0., 1.],\n",
       "         [0., 0., 1., 0.],\n",
       "         [0., 1., 0., 0.],\n",
       "         [0., 0., 1., 0.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [0., 0., 1., 0.],\n",
       "         [0., 0., 0., 1.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [0., 0., 0., 1.],\n",
       "         [0., 0., 0., 1.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [0., 0., 0., 1.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [0., 1., 0., 0.],\n",
       "         [0., 0., 1., 0.],\n",
       "         [0., 0., 1., 0.],\n",
       "         [0., 0., 0., 1.],\n",
       "         [0., 0., 0., 1.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [0., 0., 1., 0.],\n",
       "         [0., 0., 1., 0.],\n",
       "         [0., 0., 1., 0.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [0., 0., 1., 0.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [0., 0., 1., 0.],\n",
       "         [0., 0., 0., 1.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [0., 0., 1., 0.],\n",
       "         [0., 0., 0., 1.],\n",
       "         [0., 1., 0., 0.],\n",
       "         [0., 0., 1., 0.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [0., 1., 0., 0.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [0., 0., 1., 0.],\n",
       "         [0., 1., 0., 0.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [0., 0., 1., 0.],\n",
       "         [0., 1., 0., 0.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [0., 0., 0., 1.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [0., 1., 0., 0.],\n",
       "         [0., 1., 0., 0.],\n",
       "         [0., 1., 0., 0.],\n",
       "         [0., 0., 0., 1.],\n",
       "         [0., 0., 0., 1.],\n",
       "         [0., 1., 0., 0.],\n",
       "         [0., 0., 0., 1.],\n",
       "         [0., 0., 1., 0.],\n",
       "         [0., 1., 0., 0.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [0., 1., 0., 0.],\n",
       "         [0., 0., 0., 1.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [0., 1., 0., 0.],\n",
       "         [0., 1., 0., 0.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [0., 1., 0., 0.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [0., 0., 0., 1.],\n",
       "         [0., 0., 1., 0.],\n",
       "         [0., 0., 0., 1.],\n",
       "         [0., 1., 0., 0.],\n",
       "         [0., 1., 0., 0.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [0., 0., 1., 0.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [0., 0., 0., 1.],\n",
       "         [0., 1., 0., 0.],\n",
       "         [0., 0., 1., 0.],\n",
       "         [0., 0., 0., 1.],\n",
       "         [0., 1., 0., 0.],\n",
       "         [0., 0., 0., 1.],\n",
       "         [0., 0., 1., 0.],\n",
       "         [0., 1., 0., 0.],\n",
       "         [0., 0., 1., 0.],\n",
       "         [0., 0., 0., 1.],\n",
       "         [0., 1., 0., 0.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [0., 1., 0., 0.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [0., 0., 1., 0.],\n",
       "         [0., 0., 0., 1.],\n",
       "         [0., 0., 0., 1.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [0., 0., 1., 0.],\n",
       "         [0., 0., 1., 0.],\n",
       "         [0., 0., 0., 1.],\n",
       "         [0., 1., 0., 0.],\n",
       "         [0., 0., 1., 0.],\n",
       "         [0., 0., 1., 0.]])]"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "eb841ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class model(nn.Module):\n",
    "    def __init__(self,num_class,dimension):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(21*dimension,2**(dimension+2)) \n",
    "        self.Dropout1 = nn.Dropout(p = 0.2)\n",
    "        self.act1 = nn.ReLU()\n",
    "        \n",
    "        self.linear2 = nn.Linear(2**(dimension+2),2**(dimension+1))\n",
    "        self.Dropout2 = nn.Dropout(p=0.2)\n",
    "        self.act2 = nn.ReLU() \n",
    "        \n",
    "        self.linear3 = nn.Linear(2**(dimension+1),num_class)\n",
    "        self.act3 = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.linear1(x)\n",
    "        out = self.Dropout1(out)\n",
    "        out = self.act1(out)\n",
    "        out = self.linear2(out)\n",
    "        out = self.Dropout2(out)\n",
    "        out = self.act2(out)    \n",
    "        out = self.linear3(out)\n",
    "        out = self.act3(out)  \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "8ec22274",
   "metadata": {},
   "outputs": [],
   "source": [
    "class model_2(nn.Module):\n",
    "    def __init__(self,num_class,dimension):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(21*dimension,2**(dimension+3)) \n",
    "        self.Dropout1 = nn.Dropout(p = 0.2)\n",
    "        self.act1 = nn.ReLU()\n",
    "        \n",
    "        self.linear2 = nn.Linear(2**(dimension+3),2**(dimension+2))\n",
    "        self.Dropout2 = nn.Dropout(p=0.2)\n",
    "        self.act2 = nn.ReLU() \n",
    "        \n",
    "        self.linear3 = nn.Linear(2**(dimension+2),2**(dimension+1))\n",
    "        self.Dropout3 = nn.Dropout(p=0.2)\n",
    "        self.act3= nn.ReLU() \n",
    "        \n",
    "        self.linear4 = nn.Linear(2**(dimension+1),num_class)\n",
    "        self.act4 = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.linear1(x)\n",
    "        out = self.Dropout1(out)\n",
    "        out = self.act1(out)\n",
    "        out = self.linear2(out)\n",
    "        out = self.Dropout2(out)\n",
    "        out = self.act2(out)   \n",
    "        \n",
    "        out = self.linear3(out)\n",
    "        out = self.Dropout3(out)\n",
    "        out = self.act3(out)  \n",
    "        \n",
    "        out = self.linear4(out)\n",
    "        out = self.act4(out)  \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "7f7a9bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_2(num_class = 4, dimension=3)\n",
    "\n",
    "import torch.optim as optim\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.SGD(model.parameters(), lr = 0.001, momentum=0.9)\n",
    "optimizer = optim.Adam(model.parameters() , lr = 0.01)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size = 500,gamma=0.1)\n",
    "num_epochs = 5000\n",
    "loss_his = []\n",
    "test_accuracy_his = []\n",
    "test_loss_his = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "aa197bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0\n",
      "loss : 0.025841478526592254\n",
      "epoch : 1\n",
      "loss : 0.02365873920917511\n",
      "epoch : 2\n",
      "loss : 0.0227425836622715\n",
      "epoch : 3\n",
      "loss : 0.020999532073736192\n",
      "epoch : 4\n",
      "loss : 0.020359825313091277\n",
      "epoch : 5\n",
      "loss : 0.02017986187338829\n",
      "epoch : 6\n",
      "loss : 0.02012137195467949\n",
      "epoch : 7\n",
      "loss : 0.020071204006671905\n",
      "epoch : 8\n",
      "loss : 0.020062415063381196\n",
      "epoch : 9\n",
      "loss : 0.020040617197752\n",
      "epoch : 10\n",
      "loss : 0.02004632115364075\n",
      "epoch : 11\n",
      "loss : 0.02004181945323944\n",
      "epoch : 12\n",
      "loss : 0.020009951055049897\n",
      "epoch : 13\n",
      "loss : 0.020012893199920653\n",
      "epoch : 14\n",
      "loss : 0.020033317148685455\n",
      "epoch : 15\n",
      "loss : 0.020036190390586853\n",
      "epoch : 16\n",
      "loss : 0.020011236131191254\n",
      "epoch : 17\n",
      "loss : 0.020010184019804002\n",
      "epoch : 18\n",
      "loss : 0.019996546775102614\n",
      "epoch : 19\n",
      "loss : 0.018609739363193514\n",
      "epoch : 20\n",
      "loss : 0.017541762262582777\n",
      "epoch : 21\n",
      "loss : 0.017489242643117905\n",
      "epoch : 22\n",
      "loss : 0.017509134650230407\n",
      "epoch : 23\n",
      "loss : 0.017518792569637297\n",
      "epoch : 24\n",
      "loss : 0.017520867109298705\n",
      "epoch : 25\n",
      "loss : 0.01749939924478531\n",
      "epoch : 26\n",
      "loss : 0.01745857957005501\n",
      "epoch : 27\n",
      "loss : 0.017479272305965424\n",
      "epoch : 28\n",
      "loss : 0.017475774824619292\n",
      "epoch : 29\n",
      "loss : 0.017470170080661773\n",
      "epoch : 30\n",
      "loss : 0.01748113912343979\n",
      "epoch : 31\n",
      "loss : 0.01746619978547096\n",
      "epoch : 32\n",
      "loss : 0.01747964957356453\n",
      "epoch : 33\n",
      "loss : 0.0174566131234169\n",
      "epoch : 34\n",
      "loss : 0.017483025521039964\n",
      "epoch : 35\n",
      "loss : 0.017287592619657517\n",
      "epoch : 36\n",
      "loss : 0.01622278192639351\n",
      "epoch : 37\n",
      "loss : 0.016158648192882537\n",
      "epoch : 38\n",
      "loss : 0.016156319946050643\n",
      "epoch : 39\n",
      "loss : 0.01614905458688736\n",
      "epoch : 40\n",
      "loss : 0.016155292779207228\n",
      "epoch : 41\n",
      "loss : 0.016170564562082292\n",
      "epoch : 42\n",
      "loss : 0.016175529956817626\n",
      "epoch : 43\n",
      "loss : 0.01616960820555687\n",
      "epoch : 44\n",
      "loss : 0.01614862012863159\n",
      "epoch : 45\n",
      "loss : 0.016140389293432235\n",
      "epoch : 46\n",
      "loss : 0.016158476442098616\n",
      "epoch : 47\n",
      "loss : 0.016158943980932237\n",
      "epoch : 48\n",
      "loss : 0.01615274277329445\n",
      "epoch : 49\n",
      "loss : 0.016149430811405182\n",
      "epoch : 50\n",
      "loss : 0.01616190731525421\n",
      "epoch : 51\n",
      "loss : 0.01613748049736023\n",
      "epoch : 52\n",
      "loss : 0.016129750669002532\n",
      "epoch : 53\n",
      "loss : 0.016138658195734023\n",
      "epoch : 54\n",
      "loss : 0.01613800501823425\n",
      "epoch : 55\n",
      "loss : 0.01611539128422737\n",
      "epoch : 56\n",
      "loss : 0.016150127857923507\n",
      "epoch : 57\n",
      "loss : 0.016135926604270934\n",
      "epoch : 58\n",
      "loss : 0.016120418161153793\n",
      "epoch : 59\n",
      "loss : 0.016145938426256178\n",
      "epoch : 60\n",
      "loss : 0.01612410932779312\n",
      "epoch : 61\n",
      "loss : 0.01613738414645195\n",
      "epoch : 62\n",
      "loss : 0.01619257888197899\n",
      "epoch : 63\n",
      "loss : 0.01613838803768158\n",
      "epoch : 64\n",
      "loss : 0.016161898642778397\n",
      "epoch : 65\n",
      "loss : 0.016161832123994826\n",
      "epoch : 66\n",
      "loss : 0.016169362127780914\n",
      "epoch : 67\n",
      "loss : 0.016174149215221405\n",
      "epoch : 68\n",
      "loss : 0.0161479072868824\n",
      "epoch : 69\n",
      "loss : 0.016156937062740327\n",
      "epoch : 70\n",
      "loss : 0.01613509327173233\n",
      "epoch : 71\n",
      "loss : 0.01614220741391182\n",
      "epoch : 72\n",
      "loss : 0.016138533622026444\n",
      "epoch : 73\n",
      "loss : 0.016180013418197633\n",
      "epoch : 74\n",
      "loss : 0.01613558694720268\n",
      "epoch : 75\n",
      "loss : 0.01612452220916748\n",
      "epoch : 76\n",
      "loss : 0.016139073103666305\n",
      "epoch : 77\n",
      "loss : 0.016135151714086533\n",
      "epoch : 78\n",
      "loss : 0.016149762362241745\n",
      "epoch : 79\n",
      "loss : 0.016205998599529266\n",
      "epoch : 80\n",
      "loss : 0.016131735622882844\n",
      "epoch : 81\n",
      "loss : 0.01613578701019287\n",
      "epoch : 82\n",
      "loss : 0.016128786504268647\n",
      "epoch : 83\n",
      "loss : 0.016139965176582335\n",
      "epoch : 84\n",
      "loss : 0.016145467787981035\n",
      "epoch : 85\n",
      "loss : 0.016142225533723832\n",
      "epoch : 86\n",
      "loss : 0.01612408718466759\n",
      "epoch : 87\n",
      "loss : 0.016145736932754517\n",
      "epoch : 88\n",
      "loss : 0.01612708431482315\n",
      "epoch : 89\n",
      "loss : 0.016121524989604948\n",
      "epoch : 90\n",
      "loss : 0.016114383280277252\n",
      "epoch : 91\n",
      "loss : 0.01615047085285187\n",
      "epoch : 92\n",
      "loss : 0.01614713653922081\n",
      "epoch : 93\n",
      "loss : 0.016152793794870376\n",
      "epoch : 94\n",
      "loss : 0.016146081537008285\n",
      "epoch : 95\n",
      "loss : 0.01613807025551796\n",
      "epoch : 96\n",
      "loss : 0.01613638663291931\n",
      "epoch : 97\n",
      "loss : 0.016142010152339934\n",
      "epoch : 98\n",
      "loss : 0.016117266088724137\n",
      "epoch : 99\n",
      "loss : 0.016126210302114488\n",
      "test accuracy =  0.988\n",
      "epoch : 100\n",
      "loss : 0.016136520206928254\n",
      "epoch : 101\n",
      "loss : 0.016121342927217484\n",
      "epoch : 102\n",
      "loss : 0.016112991839647293\n",
      "epoch : 103\n",
      "loss : 0.016160444170236588\n",
      "epoch : 104\n",
      "loss : 0.016115752667188645\n",
      "epoch : 105\n",
      "loss : 0.016140263110399247\n",
      "epoch : 106\n",
      "loss : 0.016138158947229385\n",
      "epoch : 107\n",
      "loss : 0.016153333246707917\n",
      "epoch : 108\n",
      "loss : 0.016127059400081634\n",
      "epoch : 109\n",
      "loss : 0.01613382217288017\n",
      "epoch : 110\n",
      "loss : 0.016128063470125197\n",
      "epoch : 111\n",
      "loss : 0.01611849081516266\n",
      "epoch : 112\n",
      "loss : 0.01611035719513893\n",
      "epoch : 113\n",
      "loss : 0.016121259987354277\n",
      "epoch : 114\n",
      "loss : 0.01614479798078537\n",
      "epoch : 115\n",
      "loss : 0.016156762450933455\n",
      "epoch : 116\n",
      "loss : 0.01615202161669731\n",
      "epoch : 117\n",
      "loss : 0.016120634227991103\n",
      "epoch : 118\n",
      "loss : 0.016116284996271133\n",
      "epoch : 119\n",
      "loss : 0.01611362701654434\n",
      "epoch : 120\n",
      "loss : 0.01612944424152374\n",
      "epoch : 121\n",
      "loss : 0.01613585901260376\n",
      "epoch : 122\n",
      "loss : 0.01612666729092598\n",
      "epoch : 123\n",
      "loss : 0.016123096466064454\n",
      "epoch : 124\n",
      "loss : 0.016133863359689714\n",
      "epoch : 125\n",
      "loss : 0.01612472140789032\n",
      "epoch : 126\n",
      "loss : 0.016118584007024764\n",
      "epoch : 127\n",
      "loss : 0.016152789026498793\n",
      "epoch : 128\n",
      "loss : 0.016166938155889512\n",
      "epoch : 129\n",
      "loss : 0.01612760192155838\n",
      "epoch : 130\n",
      "loss : 0.016108278304338454\n",
      "epoch : 131\n",
      "loss : 0.016131082445383072\n",
      "epoch : 132\n",
      "loss : 0.01611573603749275\n",
      "epoch : 133\n",
      "loss : 0.016122196823358536\n",
      "epoch : 134\n",
      "loss : 0.016126243442296982\n",
      "epoch : 135\n",
      "loss : 0.016118676126003265\n",
      "epoch : 136\n",
      "loss : 0.01611544671654701\n",
      "epoch : 137\n",
      "loss : 0.016126985967159273\n",
      "epoch : 138\n",
      "loss : 0.016123603999614714\n",
      "epoch : 139\n",
      "loss : 0.016113858580589294\n",
      "epoch : 140\n",
      "loss : 0.01611983734369278\n",
      "epoch : 141\n",
      "loss : 0.01612016722559929\n",
      "epoch : 142\n",
      "loss : 0.01612248820066452\n",
      "epoch : 143\n",
      "loss : 0.016111274003982545\n",
      "epoch : 144\n",
      "loss : 0.016106078296899796\n",
      "epoch : 145\n",
      "loss : 0.016128752022981644\n",
      "epoch : 146\n",
      "loss : 0.01612001022696495\n",
      "epoch : 147\n",
      "loss : 0.01612110123038292\n",
      "epoch : 148\n",
      "loss : 0.016110857546329498\n",
      "epoch : 149\n",
      "loss : 0.016116158425807953\n",
      "epoch : 150\n",
      "loss : 0.016115965485572814\n",
      "epoch : 151\n",
      "loss : 0.016123607456684112\n",
      "epoch : 152\n",
      "loss : 0.01615198540687561\n",
      "epoch : 153\n",
      "loss : 0.01614957582950592\n",
      "epoch : 154\n",
      "loss : 0.016150889903306962\n",
      "epoch : 155\n",
      "loss : 0.016140006721019746\n",
      "epoch : 156\n",
      "loss : 0.016164096921682358\n",
      "epoch : 157\n",
      "loss : 0.01612585711479187\n",
      "epoch : 158\n",
      "loss : 0.01614189949631691\n",
      "epoch : 159\n",
      "loss : 0.016159075319767\n",
      "epoch : 160\n",
      "loss : 0.016142054677009583\n",
      "epoch : 161\n",
      "loss : 0.016137994915246964\n",
      "epoch : 162\n",
      "loss : 0.0161410653591156\n",
      "epoch : 163\n",
      "loss : 0.01611570033431053\n",
      "epoch : 164\n",
      "loss : 0.016120702207088472\n",
      "epoch : 165\n",
      "loss : 0.016135086119174958\n",
      "epoch : 166\n",
      "loss : 0.01612224894762039\n",
      "epoch : 167\n",
      "loss : 0.01612650829553604\n",
      "epoch : 168\n",
      "loss : 0.016123057156801224\n",
      "epoch : 169\n",
      "loss : 0.016102773398160936\n",
      "epoch : 170\n",
      "loss : 0.01611597689986229\n",
      "epoch : 171\n",
      "loss : 0.016121682852506636\n",
      "epoch : 172\n",
      "loss : 0.016147489845752715\n",
      "epoch : 173\n",
      "loss : 0.01611339142918587\n",
      "epoch : 174\n",
      "loss : 0.016124972462654114\n",
      "epoch : 175\n",
      "loss : 0.016129143565893175\n",
      "epoch : 176\n",
      "loss : 0.016138368427753447\n",
      "epoch : 177\n",
      "loss : 0.01609956592321396\n",
      "epoch : 178\n",
      "loss : 0.016137428492307662\n",
      "epoch : 179\n",
      "loss : 0.016216746330261232\n",
      "epoch : 180\n",
      "loss : 0.016115277230739595\n",
      "epoch : 181\n",
      "loss : 0.01611352303624153\n",
      "epoch : 182\n",
      "loss : 0.01611545166373253\n",
      "epoch : 183\n",
      "loss : 0.01611528927087784\n",
      "epoch : 184\n",
      "loss : 0.01612175899744034\n",
      "epoch : 185\n",
      "loss : 0.016109058916568757\n",
      "epoch : 186\n",
      "loss : 0.01610906609892845\n",
      "epoch : 187\n",
      "loss : 0.016137540161609648\n",
      "epoch : 188\n",
      "loss : 0.01613793784379959\n",
      "epoch : 189\n",
      "loss : 0.01615881684422493\n",
      "epoch : 190\n",
      "loss : 0.01610929799079895\n",
      "epoch : 191\n",
      "loss : 0.016101350158452987\n",
      "epoch : 192\n",
      "loss : 0.016125040739774704\n",
      "epoch : 193\n",
      "loss : 0.016132005959749222\n",
      "epoch : 194\n",
      "loss : 0.016113574773073198\n",
      "epoch : 195\n",
      "loss : 0.0161457622051239\n",
      "epoch : 196\n",
      "loss : 0.016111540049314498\n",
      "epoch : 197\n",
      "loss : 0.016149096012115477\n",
      "epoch : 198\n",
      "loss : 0.016131159722805023\n",
      "epoch : 199\n",
      "loss : 0.01612851244211197\n",
      "test accuracy =  0.99\n",
      "epoch : 200\n",
      "loss : 0.016136383026838303\n",
      "epoch : 201\n",
      "loss : 0.016117860078811646\n",
      "epoch : 202\n",
      "loss : 0.016122730761766433\n",
      "epoch : 203\n",
      "loss : 0.016127777874469756\n",
      "epoch : 204\n",
      "loss : 0.0161293506026268\n",
      "epoch : 205\n",
      "loss : 0.01612415587902069\n",
      "epoch : 206\n",
      "loss : 0.016163993865251542\n",
      "epoch : 207\n",
      "loss : 0.01614366737008095\n",
      "epoch : 208\n",
      "loss : 0.016125052750110627\n",
      "epoch : 209\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss : 0.016119475424289702\n",
      "epoch : 210\n",
      "loss : 0.016127744644880294\n",
      "epoch : 211\n",
      "loss : 0.016126222848892213\n",
      "epoch : 212\n",
      "loss : 0.016125762671232224\n",
      "epoch : 213\n",
      "loss : 0.016141136139631272\n",
      "epoch : 214\n",
      "loss : 0.016134916305541994\n",
      "epoch : 215\n",
      "loss : 0.01611670231819153\n",
      "epoch : 216\n",
      "loss : 0.01612920206785202\n",
      "epoch : 217\n",
      "loss : 0.01610923483967781\n",
      "epoch : 218\n",
      "loss : 0.016131455838680267\n",
      "epoch : 219\n",
      "loss : 0.01611448520421982\n",
      "epoch : 220\n",
      "loss : 0.016125696510076525\n",
      "epoch : 221\n",
      "loss : 0.016126909404993058\n",
      "epoch : 222\n",
      "loss : 0.01610499683022499\n",
      "epoch : 223\n",
      "loss : 0.016108520060777665\n",
      "epoch : 224\n",
      "loss : 0.016105292946100237\n",
      "epoch : 225\n",
      "loss : 0.016130921959877014\n",
      "epoch : 226\n",
      "loss : 0.016095127910375595\n",
      "epoch : 227\n",
      "loss : 0.016133203655481338\n",
      "epoch : 228\n",
      "loss : 0.016117754876613617\n",
      "epoch : 229\n",
      "loss : 0.016122345119714738\n",
      "epoch : 230\n",
      "loss : 0.016155624687671663\n",
      "epoch : 231\n",
      "loss : 0.016117473781108855\n",
      "epoch : 232\n",
      "loss : 0.01614271742105484\n",
      "epoch : 233\n",
      "loss : 0.016113861858844756\n",
      "epoch : 234\n",
      "loss : 0.016115387558937072\n",
      "epoch : 235\n",
      "loss : 0.016208429664373396\n",
      "epoch : 236\n",
      "loss : 0.016143114864826204\n",
      "epoch : 237\n",
      "loss : 0.016151047140359878\n",
      "epoch : 238\n",
      "loss : 0.016138552457094193\n",
      "epoch : 239\n",
      "loss : 0.016119179606437682\n",
      "epoch : 240\n",
      "loss : 0.01612497130036354\n",
      "epoch : 241\n",
      "loss : 0.016121186405420304\n",
      "epoch : 242\n",
      "loss : 0.016131728053092955\n",
      "epoch : 243\n",
      "loss : 0.016125971853733063\n",
      "epoch : 244\n",
      "loss : 0.01614971414208412\n",
      "epoch : 245\n",
      "loss : 0.016123238384723664\n",
      "epoch : 246\n",
      "loss : 0.01611992770433426\n",
      "epoch : 247\n",
      "loss : 0.01614126467704773\n",
      "epoch : 248\n",
      "loss : 0.016319567918777467\n",
      "epoch : 249\n",
      "loss : 0.016113296031951906\n",
      "epoch : 250\n",
      "loss : 0.016132169455289842\n",
      "epoch : 251\n",
      "loss : 0.016109786182641982\n",
      "epoch : 252\n",
      "loss : 0.016121490389108657\n",
      "epoch : 253\n",
      "loss : 0.01612946429848671\n",
      "epoch : 254\n",
      "loss : 0.01608283945918083\n",
      "epoch : 255\n",
      "loss : 0.016121888041496276\n",
      "epoch : 256\n",
      "loss : 0.016117902994155883\n",
      "epoch : 257\n",
      "loss : 0.016131302386522293\n",
      "epoch : 258\n",
      "loss : 0.016144610613584517\n",
      "epoch : 259\n",
      "loss : 0.016148607164621354\n",
      "epoch : 260\n",
      "loss : 0.016129388839006425\n",
      "epoch : 261\n",
      "loss : 0.016115215331315994\n",
      "epoch : 262\n",
      "loss : 0.01612581166625023\n",
      "epoch : 263\n",
      "loss : 0.0161098692715168\n",
      "epoch : 264\n",
      "loss : 0.016121893167495727\n",
      "epoch : 265\n",
      "loss : 0.016107327848672866\n",
      "epoch : 266\n",
      "loss : 0.016112189888954163\n",
      "epoch : 267\n",
      "loss : 0.01612093770503998\n",
      "epoch : 268\n",
      "loss : 0.016119305849075317\n",
      "epoch : 269\n",
      "loss : 0.016108009040355684\n",
      "epoch : 270\n",
      "loss : 0.016143262028694153\n",
      "epoch : 271\n",
      "loss : 0.016116196393966675\n",
      "epoch : 272\n",
      "loss : 0.016117837995290756\n",
      "epoch : 273\n",
      "loss : 0.01612242490053177\n",
      "epoch : 274\n",
      "loss : 0.01611485317349434\n",
      "epoch : 275\n",
      "loss : 0.016119569212198256\n",
      "epoch : 276\n",
      "loss : 0.016119191467761994\n",
      "epoch : 277\n",
      "loss : 0.016114300400018693\n",
      "epoch : 278\n",
      "loss : 0.016117135852575303\n",
      "epoch : 279\n",
      "loss : 0.01611637058854103\n",
      "epoch : 280\n",
      "loss : 0.016129793912172317\n",
      "epoch : 281\n",
      "loss : 0.016118611603975295\n",
      "epoch : 282\n",
      "loss : 0.0161163310110569\n",
      "epoch : 283\n",
      "loss : 0.01611046588420868\n",
      "epoch : 284\n",
      "loss : 0.016105034857988358\n",
      "epoch : 285\n",
      "loss : 0.0161113406419754\n",
      "epoch : 286\n",
      "loss : 0.016106569319963456\n",
      "epoch : 287\n",
      "loss : 0.01610594925284386\n",
      "epoch : 288\n",
      "loss : 0.01614088836312294\n",
      "epoch : 289\n",
      "loss : 0.016112963885068894\n",
      "epoch : 290\n",
      "loss : 0.01610970202088356\n",
      "epoch : 291\n",
      "loss : 0.01612897053360939\n",
      "epoch : 292\n",
      "loss : 0.01612023800611496\n",
      "epoch : 293\n",
      "loss : 0.016116577953100203\n",
      "epoch : 294\n",
      "loss : 0.01610609292984009\n",
      "epoch : 295\n",
      "loss : 0.016117985010147096\n",
      "epoch : 296\n",
      "loss : 0.01611407518386841\n",
      "epoch : 297\n",
      "loss : 0.01612499898672104\n",
      "epoch : 298\n",
      "loss : 0.01610440707206726\n",
      "epoch : 299\n",
      "loss : 0.016104792207479478\n",
      "test accuracy =  0.996\n",
      "epoch : 300\n",
      "loss : 0.01611941760778427\n",
      "epoch : 301\n",
      "loss : 0.01611747607588768\n",
      "epoch : 302\n",
      "loss : 0.01611501082777977\n",
      "epoch : 303\n",
      "loss : 0.01612059798836708\n",
      "epoch : 304\n",
      "loss : 0.016114538937807083\n",
      "epoch : 305\n",
      "loss : 0.016232821673154832\n",
      "epoch : 306\n",
      "loss : 0.0161499784886837\n",
      "epoch : 307\n",
      "loss : 0.01612534576654434\n",
      "epoch : 308\n",
      "loss : 0.016142723619937897\n",
      "epoch : 309\n",
      "loss : 0.016089876770973205\n",
      "epoch : 310\n",
      "loss : 0.016101537466049195\n",
      "epoch : 311\n",
      "loss : 0.01610564610362053\n",
      "epoch : 312\n",
      "loss : 0.016115648210048676\n",
      "epoch : 313\n",
      "loss : 0.016114179164171218\n",
      "epoch : 314\n",
      "loss : 0.016100945800542832\n",
      "epoch : 315\n",
      "loss : 0.016101802557706834\n",
      "epoch : 316\n",
      "loss : 0.016121011257171632\n",
      "epoch : 317\n",
      "loss : 0.01611990749835968\n",
      "epoch : 318\n",
      "loss : 0.01611564549803734\n",
      "epoch : 319\n",
      "loss : 0.016114004850387574\n",
      "epoch : 320\n",
      "loss : 0.016115620583295822\n",
      "epoch : 321\n",
      "loss : 0.016111047834157944\n",
      "epoch : 322\n",
      "loss : 0.01612034597992897\n",
      "epoch : 323\n",
      "loss : 0.016103329956531524\n",
      "epoch : 324\n",
      "loss : 0.016127721786499023\n",
      "epoch : 325\n",
      "loss : 0.01611355459690094\n",
      "epoch : 326\n",
      "loss : 0.016115065395832063\n",
      "epoch : 327\n",
      "loss : 0.01611074396967888\n",
      "epoch : 328\n",
      "loss : 0.016105892807245253\n",
      "epoch : 329\n",
      "loss : 0.016104388505220415\n",
      "epoch : 330\n",
      "loss : 0.016097962617874145\n",
      "epoch : 331\n",
      "loss : 0.016107452034950256\n",
      "epoch : 332\n",
      "loss : 0.016108131796121597\n",
      "epoch : 333\n",
      "loss : 0.016119566410779954\n",
      "epoch : 334\n",
      "loss : 0.01610591745376587\n",
      "epoch : 335\n",
      "loss : 0.016105309098958968\n",
      "epoch : 336\n",
      "loss : 0.016280907005071642\n",
      "epoch : 337\n",
      "loss : 0.01619659760594368\n",
      "epoch : 338\n",
      "loss : 0.016124529927968978\n",
      "epoch : 339\n",
      "loss : 0.016137651443481445\n",
      "epoch : 340\n",
      "loss : 0.01613692760467529\n",
      "epoch : 341\n",
      "loss : 0.016120178371667862\n",
      "epoch : 342\n",
      "loss : 0.016103730171918868\n",
      "epoch : 343\n",
      "loss : 0.016126133233308793\n",
      "epoch : 344\n",
      "loss : 0.01609115120768547\n",
      "epoch : 345\n",
      "loss : 0.016092602461576462\n",
      "epoch : 346\n",
      "loss : 0.01613526666164398\n",
      "epoch : 347\n",
      "loss : 0.01609406015276909\n",
      "epoch : 348\n",
      "loss : 0.016110707819461823\n",
      "epoch : 349\n",
      "loss : 0.016099286288022995\n",
      "epoch : 350\n",
      "loss : 0.016101465970277788\n",
      "epoch : 351\n",
      "loss : 0.01610178992152214\n",
      "epoch : 352\n",
      "loss : 0.016107092559337614\n",
      "epoch : 353\n",
      "loss : 0.016110222220420838\n",
      "epoch : 354\n",
      "loss : 0.01612798210978508\n",
      "epoch : 355\n",
      "loss : 0.016076976150274275\n",
      "epoch : 356\n",
      "loss : 0.01605654826760292\n",
      "epoch : 357\n",
      "loss : 0.016082718819379806\n",
      "epoch : 358\n",
      "loss : 0.0160736540555954\n",
      "epoch : 359\n",
      "loss : 0.0160759796500206\n",
      "epoch : 360\n",
      "loss : 0.016089317113161087\n",
      "epoch : 361\n",
      "loss : 0.016066605150699617\n",
      "epoch : 362\n",
      "loss : 0.016073202967643738\n",
      "epoch : 363\n",
      "loss : 0.016080481112003327\n",
      "epoch : 364\n",
      "loss : 0.016075587898492814\n",
      "epoch : 365\n",
      "loss : 0.016063297629356384\n",
      "epoch : 366\n",
      "loss : 0.01607135570049286\n",
      "epoch : 367\n",
      "loss : 0.016074044942855834\n",
      "epoch : 368\n",
      "loss : 0.016083289712667464\n",
      "epoch : 369\n",
      "loss : 0.016086930781602858\n",
      "epoch : 370\n",
      "loss : 0.01607411667704582\n",
      "epoch : 371\n",
      "loss : 0.016076387256383894\n",
      "epoch : 372\n",
      "loss : 0.016077756971120835\n",
      "epoch : 373\n",
      "loss : 0.016077596426010133\n",
      "epoch : 374\n",
      "loss : 0.016070726811885833\n",
      "epoch : 375\n",
      "loss : 0.016074623882770537\n",
      "epoch : 376\n",
      "loss : 0.016079007029533385\n",
      "epoch : 377\n",
      "loss : 0.016075728803873063\n",
      "epoch : 378\n",
      "loss : 0.016070878624916077\n",
      "epoch : 379\n",
      "loss : 0.016065727949142457\n",
      "epoch : 380\n",
      "loss : 0.016062182575464247\n",
      "epoch : 381\n",
      "loss : 0.016069462090730666\n",
      "epoch : 382\n",
      "loss : 0.016087805360555648\n",
      "epoch : 383\n",
      "loss : 0.0160714610517025\n",
      "epoch : 384\n",
      "loss : 0.01608811894059181\n",
      "epoch : 385\n",
      "loss : 0.01611766278743744\n",
      "epoch : 386\n",
      "loss : 0.01610198149085045\n",
      "epoch : 387\n",
      "loss : 0.016094645142555236\n",
      "epoch : 388\n",
      "loss : 0.016080055624246598\n",
      "epoch : 389\n",
      "loss : 0.016070997059345244\n",
      "epoch : 390\n",
      "loss : 0.016070048689842225\n",
      "epoch : 391\n",
      "loss : 0.016060574233531952\n",
      "epoch : 392\n",
      "loss : 0.016070220559835435\n",
      "epoch : 393\n",
      "loss : 0.01608613532781601\n",
      "epoch : 394\n",
      "loss : 0.016080300688743592\n",
      "epoch : 395\n",
      "loss : 0.01608683267235756\n",
      "epoch : 396\n",
      "loss : 0.016095804661512374\n",
      "epoch : 397\n",
      "loss : 0.016078641325235368\n",
      "epoch : 398\n",
      "loss : 0.016068983286619185\n",
      "epoch : 399\n",
      "loss : 0.016092259526252747\n",
      "test accuracy =  0.998\n",
      "epoch : 400\n",
      "loss : 0.01605420899391174\n",
      "epoch : 401\n",
      "loss : 0.016070232063531874\n",
      "epoch : 402\n",
      "loss : 0.016065855652093886\n",
      "epoch : 403\n",
      "loss : 0.01607464262843132\n",
      "epoch : 404\n",
      "loss : 0.016069875687360763\n",
      "epoch : 405\n",
      "loss : 0.01608097645640373\n",
      "epoch : 406\n",
      "loss : 0.016078434318304063\n",
      "epoch : 407\n",
      "loss : 0.016059677481651307\n",
      "epoch : 408\n",
      "loss : 0.01619438001513481\n",
      "epoch : 409\n",
      "loss : 0.0160830397605896\n",
      "epoch : 410\n",
      "loss : 0.016084133446216585\n",
      "epoch : 411\n",
      "loss : 0.01607954865694046\n",
      "epoch : 412\n",
      "loss : 0.01607348296046257\n",
      "epoch : 413\n",
      "loss : 0.016070128619670866\n",
      "epoch : 414\n",
      "loss : 0.016053398579359054\n",
      "epoch : 415\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [218], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m keypoint, labels \u001b[38;5;241m=\u001b[39m data\n\u001b[0;32m      6\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m----> 7\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeypoint\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(out, labels)\n\u001b[0;32m      9\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\appliedprogramming\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn [216], line 28\u001b[0m, in \u001b[0;36mmodel_2.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     25\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact2(out)   \n\u001b[0;32m     27\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear3(out)\n\u001b[1;32m---> 28\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDropout3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact3(out)  \n\u001b[0;32m     31\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear4(out)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\appliedprogramming\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\appliedprogramming\\lib\\site-packages\\torch\\nn\\modules\\dropout.py:58\u001b[0m, in \u001b[0;36mDropout.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m---> 58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\appliedprogramming\\lib\\site-packages\\torch\\nn\\functional.py:1252\u001b[0m, in \u001b[0;36mdropout\u001b[1;34m(input, p, training, inplace)\u001b[0m\n\u001b[0;32m   1250\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[0;32m   1251\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout probability has to be between 0 and 1, \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(p))\n\u001b[1;32m-> 1252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mdropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    print(\"epoch : {}\".format(epoch))\n",
    "    for i,data in enumerate(train_loader):\n",
    "        keypoint, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        out = model(keypoint)\n",
    "        loss = criterion(out, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_his.append(loss.item())\n",
    "        running_loss += loss.item()\n",
    "    print(\"loss : {}\".format(running_loss/2000))\n",
    "    if epoch%100 == 99:\n",
    "        pred_test = model(test_set[:][0])\n",
    "        correct = 0\n",
    "        for i in range(len(test_set)):\n",
    "            if pred_test[i].argmax() == test_set[i][1].argmax() :\n",
    "                correct = correct+1\n",
    "        print(\"test accuracy = \", correct/len(test_set))\n",
    "        test_accuracy_his.append(correct/len(test_set))\n",
    "    scheduler.step()\n",
    "#         test_loss_his.append(criterion(pred_test, test_set[i][1]))\n",
    "        \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "188cc4df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2894a8b55a0>]"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot([i for i in range(len(loss_his))],loss_his)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "1d4dd3a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.996"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_test = model(test_set[:][0])\n",
    "correct = 0\n",
    "for i in range(len(test_set)):\n",
    "    if pred_test[i].argmax() == test_set[i][1].argmax() :\n",
    "        correct = correct+1\n",
    "correct/len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "1c666f97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9972727272727273"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_train = model(train_set[:][0])\n",
    "correct = 0\n",
    "for i in range(len(train_set)):\n",
    "    if pred_train[i].argmax() == train_set[i][1].argmax() :\n",
    "        correct = correct+1\n",
    "accuracy = correct / len(train_set)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "5920d70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),\"./custom_model_4.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "f56f1c4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.0000, -0.0000, -0.0000, -0.0295,  0.0169,  0.0023, -0.0432,  0.0403,\n",
       "         -0.0014, -0.0474,  0.0619, -0.0042, -0.0483,  0.0769, -0.0080, -0.0281,\n",
       "          0.0657, -0.0188, -0.0314,  0.0882, -0.0206, -0.0324,  0.0994, -0.0183,\n",
       "         -0.0324,  0.1098, -0.0159, -0.0145,  0.0685, -0.0188, -0.0183,  0.0929,\n",
       "         -0.0216, -0.0202,  0.1055, -0.0183, -0.0225,  0.1154, -0.0150, -0.0019,\n",
       "          0.0680, -0.0164, -0.0052,  0.0905, -0.0188, -0.0084,  0.1023, -0.0164,\n",
       "         -0.0117,  0.1121, -0.0141,  0.0127,  0.0638, -0.0136,  0.0098,  0.0811,\n",
       "         -0.0159,  0.0070,  0.0910, -0.0169,  0.0038,  0.0994, -0.0169]),\n",
       " tensor([0., 1., 0., 0.]))"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a028f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test[4].argmax() == test_set[4][1].argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c839f2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from customhandrec import model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000feee8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
